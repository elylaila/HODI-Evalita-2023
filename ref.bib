
@misc{devlin_bert_2019,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), {BERT} is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	number = {{arXiv}:1810.04805},
	publisher = {{arXiv}},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2023-08-28},
	date = {2019-05-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language, notion},
	file = {Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:C\:\\Users\\irene\\Zotero\\storage\\QCN6BVNL\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@article{hochreiter_long_1997,
	title = {Long Short-Term Memory},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory ({LSTM}). Truncating the gradient where this does not do harm, {LSTM} can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. {LSTM} is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, {LSTM} leads to many more successful runs, and learns much faster. {LSTM} also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	pages = {1735--1780},
	number = {8},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	urldate = {2023-08-28},
	date = {1997-11-15},
	keywords = {notion},
	file = {2604.pdf:C\:\\Users\\irene\\Zotero\\storage\\636YHCJM\\2604.pdf:application/pdf},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is All you Need},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 {BLEU} {onEnglish}-to-German translation, improving over the existing best ensemble result by over 1 {BLEU}. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 {BLEU}, achieving a {BLEU} score of 41.1.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	urldate = {2023-09-23},
	date = {2017},
	file = {Full Text PDF:C\:\\Users\\irene\\Zotero\\storage\\FJK67755\\Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@misc{sanh_distilbert_2020,
	title = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
	url = {http://arxiv.org/abs/1910.01108},
	doi = {10.48550/arXiv.1910.01108},
	shorttitle = {{DistilBERT}, a distilled version of {BERT}},
	abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing ({NLP}), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called {DistilBERT}, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a {BERT} model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
	number = {{arXiv}:1910.01108},
	publisher = {{arXiv}},
	author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	urldate = {2023-09-23},
	date = {2020-02-29},
	eprinttype = {arxiv},
	eprint = {1910.01108 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Sanh et al. - 2020 - DistilBERT, a distilled version of BERT smaller, .pdf:C\:\\Users\\irene\\Zotero\\storage\\I7YEA6IY\\Sanh et al. - 2020 - DistilBERT, a distilled version of BERT smaller, .pdf:application/pdf},
}

@article{mathew_hatexplain_2021,
	title = {{HateXplain}: A Benchmark Dataset for Explainable Hate Speech Detection},
	volume = {35},
	rights = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17745},
	doi = {10.1609/aaai.v35i17.17745},
	shorttitle = {{HateXplain}},
	abstract = {Hate speech is a challenging issue plaguing the online social media. While better models for hate speech detection are continuously being developed, there is little research on the bias and interpretability aspects of hate speech. In this paper, we introduce {HateXplain}, the first benchmark hate speech dataset covering multiple aspects of the issue. Each post in our dataset is annotated from three different perspectives: the basic, commonly used 3-class classification (i.e., hate, offensive or normal), the target community (i.e., the community that has been the victim of hate speech/offensive speech in the post), and the rationales, i.e., the portions of the post on which their labelling decision (as hate, offensive or normal) is based. We utilize existing state-of-the-art models and observe that even models that perform very well in classification do not score high on explainability metrics like model plausibility and faithfulness. We also observe that models, which utilize the human rationales for training, perform better in reducing unintended bias towards target communities. We have made our code and dataset public for other researchers.},
	pages = {14867--14875},
	number = {17},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Mathew, Binny and Saha, Punyajoy and Yimam, Seid Muhie and Biemann, Chris and Goyal, Pawan and Mukherjee, Animesh},
	urldate = {2023-09-23},
	date = {2021-05-18},
	langid = {english},
	note = {Number: 17},
	keywords = {Other Social Impact},
	file = {Mathew et al. - 2021 - HateXplain A Benchmark Dataset for Explainable Ha.pdf:C\:\\Users\\irene\\Zotero\\storage\\AVBE6EKD\\17745-Article Text-21239-1-2-20210518.pdf:application/pdf},
}

@misc{chaudhary_countering_2021,
	title = {Countering Online Hate Speech: An {NLP} Perspective},
	url = {http://arxiv.org/abs/2109.02941},
	doi = {10.48550/arXiv.2109.02941},
	shorttitle = {Countering Online Hate Speech},
	abstract = {Online hate speech has caught everyone's attention from the news related to the {COVID}-19 pandemic, {US} elections, and worldwide protests. Online toxicity - an umbrella term for online hateful behavior, manifests itself in forms such as online hate speech. Hate speech is a deliberate attack directed towards an individual or a group motivated by the targeted entity's identity or opinions. The rising mass communication through social media further exacerbates the harmful consequences of online hate speech. While there has been significant research on hate-speech identification using Natural Language Processing ({NLP}), the work on utilizing {NLP} for prevention and intervention of online hate speech lacks relatively. This paper presents a holistic conceptual framework on hate-speech {NLP} countering methods along with a thorough survey on the current progress of {NLP} for countering online hate speech. It classifies the countering techniques based on their time of action, and identifies potential future research areas on this topic.},
	number = {{arXiv}:2109.02941},
	publisher = {{arXiv}},
	author = {Chaudhary, Mudit and Saxena, Chandni and Meng, Helen},
	urldate = {2023-09-23},
	date = {2021-09-07},
	eprinttype = {arxiv},
	eprint = {2109.02941 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\irene\\Zotero\\storage\\8TLWBG8J\\Chaudhary et al. - 2021 - Countering Online Hate Speech An NLP Perspective.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\irene\\Zotero\\storage\\5Z4ZMXPT\\2109.html:text/html},
}

@misc{chakravarthi_dataset_2021,
	title = {Dataset for Identification of Homophobia and Transophobia in Multilingual {YouTube} Comments},
	url = {http://arxiv.org/abs/2109.00227},
	abstract = {The increased proliferation of abusive content on social media platforms has a negative impact on online users. The dread, dislike, discomfort, or mistrust of lesbian, gay, transgender or bisexual persons is defined as homophobia/transphobia. Homophobic/transphobic speech is a type of offensive language that may be summarized as hate speech directed toward {LGBT}+ people, and it has been a growing concern in recent years. Online homophobia/transphobia is a severe societal problem that can make online platforms poisonous and unwelcome to {LGBT}+ people while also attempting to eliminate equality, diversity, and inclusion. We provide a new hierarchical taxonomy for online homophobia and transphobia, as well as an expert-labelled dataset that will allow homophobic/transphobic content to be automatically identified. We educated annotators and supplied them with comprehensive annotation rules because this is a sensitive issue, and we previously discovered that untrained crowdsourcing annotators struggle with diagnosing homophobia due to cultural and other prejudices. The dataset comprises 15,141 annotated multilingual comments. This paper describes the process of building the dataset, qualitative analysis of data, and inter-annotator agreement. In addition, we create baseline models for the dataset. To the best of our knowledge, our dataset is the first such dataset created. Warning: This paper contains explicit statements of homophobia, transphobia, stereotypes which may be distressing to some readers.},
	number = {{arXiv}:2109.00227},
	publisher = {{arXiv}},
	author = {Chakravarthi, Bharathi Raja and Priyadharshini, Ruba and Ponnusamy, Rahul and Kumaresan, Prasanna Kumar and Sampath, Kayalvizhi and Thenmozhi, Durairaj and Thangasamy, Sathiyaraj and Nallathambi, Rajendran and {McCrae}, John Phillip},
	urldate = {2023-09-23},
	date = {2021-09-01},
	eprinttype = {arxiv},
	eprint = {2109.00227 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\irene\\Zotero\\storage\\NGPPSXGL\\2109.html:text/html;Full Text PDF:C\:\\Users\\irene\\Zotero\\storage\\IRGKMECJ\\Chakravarthi et al. - 2021 - Dataset for Identification of Homophobia and Trans.pdf:application/pdf},
}

@inproceedings{nozza_exposing_2021,
	location = {Online},
	title = {Exposing the limits of Zero-shot Cross-lingual Hate Speech Detection},
	url = {https://aclanthology.org/2021.acl-short.114},
	doi = {10.18653/v1/2021.acl-short.114},
	abstract = {Reducing and counter-acting hate speech on Social Media is a significant concern. Most of the proposed automatic methods are conducted exclusively on English and very few consistently labeled, non-English resources have been proposed. Learning to detect hate speech on English and transferring to unseen languages seems an immediate solution. This work is the first to shed light on the limits of this zero-shot, cross-lingual transfer learning framework for hate speech detection. We use benchmark data sets in English, Italian, and Spanish to detect hate speech towards immigrants and women. Investigating post-hoc explanations of the model, we discover that non-hateful, language-specific taboo interjections are misinterpreted as signals of hate speech. Our findings demonstrate that zero-shot, cross-lingual models cannot be used as they are, but need to be carefully designed.},
	eventtitle = {{ACL}-{IJCNLP} 2021},
	pages = {907--914},
	booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Nozza, Debora},
	urldate = {2023-09-23},
	date = {2021-08},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\irene\\Zotero\\storage\\H8PE6K96\\Nozza - 2021 - Exposing the limits of Zero-shot Cross-lingual Hat.pdf:application/pdf},
}

@inproceedings{attanasio_entropy-based_2022,
	location = {Dublin, Ireland},
	title = {Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists},
	url = {https://aclanthology.org/2022.findings-acl.88},
	doi = {10.18653/v1/2022.findings-acl.88},
	abstract = {Natural Language Processing ({NLP}) models risk overfitting to specific terms in the training data, thereby reducing their performance, fairness, and generalizability. E.g., neural hate speech detection models are strongly influenced by identity terms like gay, or women, resulting in false positives, severe unintended bias, and lower performance. Most mitigation techniques use lists of identity terms or samples from the target domain during training. However, this approach requires a-priori knowledge and introduces further bias if important terms are neglected. Instead, we propose a knowledge-free Entropy-based Attention Regularization ({EAR}) to discourage overfitting to training-specific terms. An additional objective function penalizes tokens with low self-attention entropy. We fine-tune {BERT} via {EAR}: the resulting model matches or exceeds state-of-the-art performance for hate speech classification and bias metrics on three benchmark corpora in English and Italian.{EAR} also reveals overfitting terms, i.e., terms most likely to induce bias, to help identify their effect on the model, task, and predictions.},
	eventtitle = {Findings 2022},
	pages = {1105--1119},
	booktitle = {Findings of the Association for Computational Linguistics: {ACL} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Attanasio, Giuseppe and Nozza, Debora and Hovy, Dirk and Baralis, Elena},
	urldate = {2023-09-23},
	date = {2022-05},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\irene\\Zotero\\storage\\HFW9D3H9\\Attanasio et al. - 2022 - Entropy-based Attention Regularization Frees Unint.pdf:application/pdf},
}

@inproceedings{jin_towards_2019,
	title = {Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models},
	url = {https://openreview.net/forum?id=BkxRRkSKwr},
	shorttitle = {Towards Hierarchical Importance Attribution},
	abstract = {The impressive performance of neural networks on natural language processing tasks attributes to their ability to model complicated word and phrase compositions. To explain how the model handles semantic compositions, we study hierarchical explanation of neural network predictions. We identify non-additivity and context independent importance attributions within hierarchies as two desirable properties for highlighting word and phrase compositions. We show some prior efforts on hierarchical explanations, e.g. contextual decomposition, do not satisfy the desired properties mathematically, leading to inconsistent explanation quality in different models. In this paper, we start by proposing a formal and general way to quantify the importance of each word and phrase. Following the formulation, we propose Sampling and Contextual Decomposition ({SCD}) algorithm and Sampling and Occlusion ({SOC}) algorithm. Human and metrics evaluation on both {LSTM} models and {BERT} Transformer models on multiple datasets show that our algorithms outperform prior hierarchical explanation algorithms. Our algorithms help to visualize semantic composition captured by models, extract classification rules and improve human trust of models.},
	eventtitle = {International Conference on Learning Representations},
	author = {Jin, Xisen and Wei, Zhongyu and Du, Junyi and Xue, Xiangyang and Ren, Xiang},
	urldate = {2023-10-02},
	date = {2019-09-25},
	file = {Jin et al. - 2019 - Towards Hierarchical Importance Attribution Explaining Compositional Semantics for Neural Sequence Models.pdf:C\:\\Users\\irene\\Zotero\\storage\\PCWIUCXP\\Jin et al. - 2019 - Towards Hierarchical Importance Attribution Explaining Compositional Semantics for Neural Sequence Models.pdf:application/pdf},
}

@article{fortuna_survey_2018,
	title = {A Survey on Automatic Detection of Hate Speech in Text},
	volume = {51},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/3232676},
	doi = {10.1145/3232676},
	abstract = {The scientific study of hate speech, from a computer science point of view, is recent. This survey organizes and describes the current state of the field, providing a structured overview of previous approaches, including core algorithms, methods, and main features used. This work also discusses the complexity of the concept of hate speech, defined in many platforms and contexts, and provides a unifying definition. This area has an unquestionable potential for societal impact, particularly in online communities and digital media platforms. The development and systematization of shared resources, such as guidelines, annotated datasets in multiple languages, and algorithms, is a crucial step in advancing the automatic detection of hate speech.},
	pages = {85:1--85:30},
	number = {4},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Fortuna, Paula and Nunes, Sérgio},
	urldate = {2023-10-02},
	date = {2018-07-31},
	keywords = {Hate speech, literature review, natural language processing, opinion mining, text mining, notion},
	file = {Fortuna and Nunes - 2018 - A Survey on Automatic Detection of Hate Speech in Text.pdf:C\:\\Users\\irene\\Zotero\\storage\\7T6ACEHW\\Fortuna and Nunes - 2018 - A Survey on Automatic Detection of Hate Speech in Text.pdf:application/pdf},
}

@inproceedings{dixon_measuring_2018,
	location = {New Orleans {LA} {USA}},
	title = {Measuring and Mitigating Unintended Bias in Text Classification},
	isbn = {978-1-4503-6012-8},
	url = {https://dl.acm.org/doi/10.1145/3278721.3278729},
	doi = {10.1145/3278721.3278729},
	eventtitle = {{AIES} '18: {AAAI}/{ACM} Conference on {AI}, Ethics, and Society},
	pages = {67--73},
	booktitle = {Proceedings of the 2018 {AAAI}/{ACM} Conference on {AI}, Ethics, and Society},
	publisher = {{ACM}},
	author = {Dixon, Lucas and Li, John and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
	urldate = {2023-10-14},
	date = {2018-12-27},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\irene\\Zotero\\storage\\ANH6DM99\\Dixon et al. - 2018 - Measuring and Mitigating Unintended Bias in Text C.pdf:application/pdf},
}

@inproceedings{park_one-step_2017,
	location = {Vancouver, {BC}, Canada},
	title = {One-step and Two-step Classification for Abusive Language Detection on Twitter},
	url = {https://aclanthology.org/W17-3006},
	doi = {10.18653/v1/W17-3006},
	abstract = {Automatic abusive language detection is a difficult but important task for online social media. Our research explores a two-step approach of performing classification on abusive language and then classifying into specific types and compares it with one-step approach of doing one multi-class classification for detecting sexist and racist languages. With a public English Twitter corpus of 20 thousand tweets in the type of sexism and racism, our approach shows a promising performance of 0.827 F-measure by using {HybridCNN} in one-step and 0.824 F-measure by using logistic regression in two-steps.},
	eventtitle = {{ALW} 2017},
	pages = {41--45},
	publisher = {Association for Computational Linguistics},
	author = {Park, Ji Ho and Fung, Pascale},
	urldate = {2023-10-20},
	date = {2017-08},
	file = {Park and Fung - 2017 - One-step and Two-step Classification for Abusive Language Detection on Twitter.pdf:C\:\\Users\\irene\\Zotero\\storage\\GU6F3PLN\\Park and Fung - 2017 - One-step and Two-step Classification for Abusive Language Detection on Twitter.pdf:application/pdf},
}

@inproceedings{badjatiya_deep_2017,
	location = {Republic and Canton of Geneva, {CHE}},
	title = {Deep Learning for Hate Speech Detection in Tweets},
	isbn = {978-1-4503-4914-7},
	url = {https://dl.acm.org/doi/10.1145/3041021.3054223},
	doi = {10.1145/3041021.3054223},
	series = {{WWW} '17 Companion},
	abstract = {Hate speech detection on Twitter is critical for applications like controversial event extraction, building {AI} chatterbots, content recommendation, and sentiment analysis. We define this task as being able to classify a tweet as racist, sexist or neither. The complexity of the natural language constructs makes this task very challenging. We perform extensive experiments with multiple deep learning architectures to learn semantic word embeddings to handle this complexity. Our experiments on a benchmark dataset of 16K annotated tweets show that such deep learning methods outperform state-of-the-art char/word n-gram methods by {\textasciitilde}18 F1 points.},
	pages = {759--760},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Badjatiya, Pinkesh and Gupta, Shashank and Gupta, Manish and Varma, Vasudeva},
	urldate = {2023-10-20},
	date = {2017-04-03},
	keywords = {cnn, deep learning applications, hate speech detection, lstm, twitter},
	file = {Badjatiya et al. - 2017 - Deep Learning for Hate Speech Detection in Tweets.pdf:C\:\\Users\\irene\\Zotero\\storage\\X5QGAZQ6\\Badjatiya et al. - 2017 - Deep Learning for Hate Speech Detection in Tweets.pdf:application/pdf},
}

@inproceedings{zampieri_semeval-2020_2020,
	location = {Barcelona (online)},
	title = {{SemEval}-2020 Task 12: Multilingual Offensive Language Identification in Social Media ({OffensEval} 2020)},
	url = {https://aclanthology.org/2020.semeval-1.188},
	doi = {10.18653/v1/2020.semeval-1.188},
	shorttitle = {{SemEval}-2020 Task 12},
	abstract = {We present the results and the main findings of {SemEval}-2020 Task 12 on Multilingual Offensive Language Identification in Social Media ({OffensEval}-2020). The task included three subtasks corresponding to the hierarchical taxonomy of the {OLID} schema from {OffensEval}-2019, and it was offered in five languages: Arabic, Danish, English, Greek, and Turkish. {OffensEval}-2020 was one of the most popular tasks at {SemEval}-2020, attracting a large number of participants across all subtasks and languages: a total of 528 teams signed up to participate in the task, 145 teams submitted official runs on the test data, and 70 teams submitted system description papers.},
	eventtitle = {{SemEval} 2020},
	pages = {1425--1447},
	publisher = {International Committee for Computational Linguistics},
	author = {Zampieri, Marcos and Nakov, Preslav and Rosenthal, Sara and Atanasova, Pepa and Karadzhov, Georgi and Mubarak, Hamdy and Derczynski, Leon and Pitenis, Zeses and Çöltekin, Çağrı},
	urldate = {2023-10-20},
	date = {2020-12},
	file = {Zampieri et al. - 2020 - SemEval-2020 Task 12 Multilingual Offensive Language Identification in Social Media (OffensEval 2020).pdf:C\:\\Users\\irene\\Zotero\\storage\\VDDAGGCK\\Zampieri et al. - 2020 - SemEval-2020 Task 12 Multilingual Offensive Language Identification in Social Media (OffensEval 2020).pdf:application/pdf},
}

@inproceedings{polignano_alberto_2019,
	title = {{AlBERTo}: Italian {BERT} Language Understanding Model for {NLP} Challenging Tasks Based on Tweets},
	url = {https://www.semanticscholar.org/paper/AlBERTo%3A-Italian-BERT-Language-Understanding-Model-Polignano-Basile/e1e43d6bdb1419e08af833cf4899a460f70da26c},
	shorttitle = {{AlBERTo}},
	abstract = {English. Recent scientific studies on natural language processing ({NLP}) report the outstanding effectiveness observed in the use of context-dependent and task-free language understanding models such as {ELMo}, {GPT}, and {BERT}. Specifically, they have proved to achieve state of the art performance in numerous complex {NLP} tasks such as question answering and sentiment analysis in the English language. Following the great popularity and effectiveness that these models are gaining in the scientific community, we trained a {BERT} language understanding model for the Italian language ({AlBERTo}). In particular, {AlBERTo} is focused on the language used in social networks, specifically on Twitter. To demonstrate its robustness, we evaluated {AlBERTo} on the {EVALITA} 2016 task {SENTIPOLC} ({SENTIment} {POLarity} Classification) obtaining state of the art results in subjectivity, polarity and irony detection on Italian tweets. The pre-trained {AlBERTo} model will be publicly distributed through the {GitHub} platform at the following web address: https://github.com/ marcopoli/{AlBERTo}-it in order to facilitate future research.},
	eventtitle = {Italian Conference on Computational Linguistics},
	author = {Polignano, Marco and Basile, Pierpaolo and Degemmis, M. and Semeraro, G. and Basile, Valerio},
	urldate = {2023-10-20},
	date = {2019},
	file = {Polignano et al. - 2019 - AlBERTo Italian BERT Language Understanding Model for NLP Challenging Tasks Based on Tweets.pdf:C\:\\Users\\irene\\Zotero\\storage\\BBHWXY78\\Polignano et al. - 2019 - AlBERTo Italian BERT Language Understanding Model for NLP Challenging Tasks Based on Tweets.pdf:application/pdf},
}

@article{polignano_alberto_2019-1,
	title = {{AlBERTo}: Modeling Italian Social Media Language with {BERT}},
	volume = {5},
	rights = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
	issn = {2499-4553},
	url = {https://journals.openedition.org/ijcol/472},
	doi = {10.4000/ijcol.472},
	shorttitle = {{AlBERTo}},
	abstract = {Natural Language Processing tasks recently achieved considerable interest and progresses following the development of numerous innovative artificial intelligence models released in recent years. The increase in available computing power has made possible the application of machine learning approaches on a considerable amount of textual data, demonstrating how they can obtain very encouraging results in challenging {NLP} tasks by generalizing the properties of natural language directly from the data. Models such as {ELMo}, {GPT}/{GPT}-2, {BERT}, {ERNIE}, and {RoBERTa} have proved to be extremely useful in {NLP} tasks such as entailment, sentiment analysis, and question answering. The availability of these resources mainly in the English language motivated us towards the realization of {AlBERTo}, a natural language model based on {BERT} and trained on the Italian language. We decided to train {AlBERTo} from scratch on social network language, Twitter in particular, because many of the classic tasks of content analysis are oriented to data extracted from the digital sphere of users. The model was distributed to the community through a repository on {GitHub} and the Transformers library (Wolf et al. 2019) released by the development group huggingface.co. We have evaluated the validity of the model on the classification tasks of sentiment polarity, irony, subjectivity, and hate speech. The specifications of the model, the code developed for training and fine-tuning, and the instructions for using it in a research project are freely available.},
	pages = {11--31},
	number = {2},
	journaltitle = {{IJCoL}. Italian Journal of Computational Linguistics},
	author = {Polignano, Marco and Basile, Valerio and Basile, Pierpaolo and de Gemmis, Marco and Semeraro, Giovanni},
	urldate = {2023-10-20},
	date = {2019-12-01},
	langid = {english},
	file = {Polignano et al. - 2019 - AlBERTo Modeling Italian Social Media Language with BERT.pdf:C\:\\Users\\irene\\Zotero\\storage\\TP2GVBIP\\Polignano et al. - 2019 - AlBERTo Modeling Italian Social Media Language with BERT.pdf:application/pdf},
}

@article{jahan_systematic_2023,
	title = {A systematic review of hate speech automatic detection using natural language processing},
	volume = {546},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231223003557},
	doi = {10.1016/j.neucom.2023.126232},
	abstract = {With the multiplication of social media platforms, which offer anonymity, easy access and online community formation and online debate, the issue of hate speech detection and tracking becomes a growing challenge to society, individual, policy-makers and researchers. Despite efforts for leveraging automatic techniques for automatic detection and monitoring, their performances are still far from satisfactory, which constantly calls for future research on the issue. This paper provides a systematic review of literature in this field, with a focus on natural language processing and deep learning technologies, highlighting the terminology, processing pipeline, core methods employed, with a focal point on deep learning architecture. From a methodological perspective, we adopt {PRISMA} guideline of systematic review of the last 10 years literature from {ACM} Digital Library and Google Scholar. In the sequel, existing surveys, limitations, and future research directions are extensively discussed.},
	pages = {126232},
	journaltitle = {Neurocomputing},
	shortjournal = {Neurocomputing},
	author = {Jahan, Md Saroar and Oussalah, Mourad},
	urldate = {2023-10-15},
	date = {2023-08-14},
	keywords = {notion, Hate speech detection review, {NLP} deep learning review, {PRISMA} hate speech, Systematic review},
	file = {main.pdf:C\:\\Users\\irene\\Zotero\\storage\\3KSGG3UY\\main.pdf:application/pdf},
}

@inreference{nockleby_hate_2000,
	title = {Hate speech},
	booktitle = {Encyclopedia of the American constitution},
	author = {Nockleby, J.T.},
	date = {2000},
}

@report{noauthor_notitle_nodate,
}

@article{noauthor_facebook_2023,
	title = {Facebook to pay \$52m to content moderators over {PTSD}},
	url = {https://www.bbc.com/news/technology-52642633},
	date = {2023-05-13},
}

@article{nozza_hodi_2023,
	title = {{HODI} at {EVALITA} 2023: Overview of the Homotransphobia Detection in Italian Task},
	journaltitle = {Proceedings of the Eighth Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. Final Workshop ({EVALITA} 2023)},
	author = {Nozza, Debora and Cignarella, Alessandra Teresa and Damo, Greta and Caselli, Tommaso and Patti, Viviana},
	date = {2023-09},
}

@online{noauthor_acnp_nodate,
	title = {{ACNP} - Catalogo Italiano Periodici},
	url = {https://acnpsearch.unibo.it/OpenURL?id=tisearch%3Ati-ex&sid=google&rft.auinit=Y&rft.aulast=Zhang&rft.atitle=Understanding+bag-of-words+model%3A+a+statistical+framework&rft.title=International+journal+of+machine+learning+and+cybernetics+%28Internet%29&rft.volume=1&rft.date=2010&rft.spage=43&rft.issn=1868-808X},
	urldate = {2023-12-21},
	keywords = {notion},
	file = {ACNP - Catalogo Italiano Periodici:C\:\\Users\\irene\\Zotero\\storage\\YYI68WYD\\OpenURL.html:text/html},
}

@misc{aluru_deep_2020,
	title = {Deep Learning Models for Multilingual Hate Speech Detection},
	url = {http://arxiv.org/abs/2004.06465},
	abstract = {Hate speech detection is a challenging problem with most of the datasets available in only one language: English. In this paper, we conduct a large scale analysis of multilingual hate speech in 9 languages from 16 different sources. We observe that in low resource setting, simple models such as {LASER} embedding with logistic regression performs the best, while in high resource setting {BERT} based models perform better. In case of zero-shot classification, languages such as Italian and Portuguese achieve good results. Our proposed framework could be used as an efficient solution for low-resource languages. These models could also act as good baselines for future multilingual hate speech detection tasks. We have made our code and experimental settings public for other researchers at https://github.com/punyajoy/{DE}-{LIMIT}.},
	number = {{arXiv}:2004.06465},
	publisher = {{arXiv}},
	author = {Aluru, Sai Saketh and Mathew, Binny and Saha, Punyajoy and Mukherjee, Animesh},
	urldate = {2023-12-21},
	date = {2020-12-09},
	eprinttype = {arxiv},
	eprint = {2004.06465 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Social and Information Networks},
	file = {arXiv.org Snapshot:C\:\\Users\\irene\\Zotero\\storage\\33YN3MB6\\2004.html:text/html;Full Text PDF:C\:\\Users\\irene\\Zotero\\storage\\LNN2DXIJ\\Aluru et al. - 2020 - Deep Learning Models for Multilingual Hate Speech .pdf:application/pdf},
}

@misc{li_survey_2021,
	title = {A Survey on Text Classification: From Shallow to Deep Learning},
	url = {http://arxiv.org/abs/2008.00364},
	doi = {10.48550/arXiv.2008.00364},
	shorttitle = {A Survey on Text Classification},
	abstract = {Text classification is the most fundamental and essential task in natural language processing. The last decade has seen a surge of research in this area due to the unprecedented success of deep learning. Numerous methods, datasets, and evaluation metrics have been proposed in the literature, raising the need for a comprehensive and updated survey. This paper fills the gap by reviewing the state-of-the-art approaches from 1961 to 2021, focusing on models from traditional models to deep learning. We create a taxonomy for text classification according to the text involved and the models used for feature extraction and classification. We then discuss each of these categories in detail, dealing with both the technical developments and benchmark datasets that support tests of predictions. A comprehensive comparison between different techniques, as well as identifying the pros and cons of various evaluation metrics are also provided in this survey. Finally, we conclude by summarizing key implications, future research directions, and the challenges facing the research area.},
	number = {{arXiv}:2008.00364},
	publisher = {{arXiv}},
	author = {Li, Qian and Peng, Hao and Li, Jianxin and Xia, Congying and Yang, Renyu and Sun, Lichao and Yu, Philip S. and He, Lifang},
	urldate = {2023-12-19},
	date = {2021-12-22},
	eprinttype = {arxiv},
	eprint = {2008.00364 [cs]},
	keywords = {Computer Science - Computation and Language, notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\irene\\Zotero\\storage\\6UULGTN5\\Li et al. - 2021 - A Survey on Text Classification From Shallow to D.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\irene\\Zotero\\storage\\PQLRXEGJ\\2008.html:text/html},
}

@misc{mikolov_efficient_2013,
	title = {Efficient Estimation of Word Representations in Vector Space},
	url = {http://arxiv.org/abs/1301.3781},
	doi = {10.48550/arXiv.1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	number = {{arXiv}:1301.3781},
	publisher = {{arXiv}},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	urldate = {2023-12-18},
	date = {2013-09-06},
	eprinttype = {arxiv},
	eprint = {1301.3781 [cs]},
	keywords = {Computer Science - Computation and Language, notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\irene\\Zotero\\storage\\974J4SYE\\Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\irene\\Zotero\\storage\\2W3BS33C\\1301.html:text/html},
}

@online{chiusano_brief_2022,
	title = {A Brief Timeline of {NLP}},
	url = {https://medium.com/nlplanet/a-brief-timeline-of-nlp-bc45b640f07d},
	abstract = {A journey across grammars, expert systems, ontologies, statistical models, neural networks, word embeddings, transformers, etc.},
	titleaddon = {{NLPlanet}},
	author = {Chiusano, Fabio},
	urldate = {2023-12-18},
	date = {2022-09-20},
	langid = {english},
}

@misc{dai_semi-supervised_2015,
	title = {Semi-supervised Sequence Learning},
	url = {http://arxiv.org/abs/1511.01432},
	abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as {IMDB}, {DBpedia} and 20 Newsgroups.},
	number = {{arXiv}:1511.01432},
	publisher = {{arXiv}},
	author = {Dai, Andrew M. and Le, Quoc V.},
	urldate = {2023-12-18},
	date = {2015-11-04},
	eprinttype = {arxiv},
	eprint = {1511.01432 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\irene\\Zotero\\storage\\FL6CYT8S\\1511.html:text/html;Full Text PDF:C\:\\Users\\irene\\Zotero\\storage\\N5F8F28I\\Dai e Le - 2015 - Semi-supervised Sequence Learning.pdf:application/pdf},
}

@online{noauthor_nlps_2018,
	title = {{NLP}'s {ImageNet} moment has arrived},
	url = {https://www.ruder.io/nlp-imagenet/},
	abstract = {Big changes are underway in the world of {NLP}. The long reign of word vectors as {NLP}'s core representation technique has seen an exciting new line of challengers emerge. These approaches demonstrated that pretrained language models can achieve state-of-the-art results and herald a watershed moment.},
	titleaddon = {ruder.io},
	urldate = {2023-12-18},
	date = {2018-07-12},
	langid = {english},
	file = {Snapshot:C\:\\Users\\irene\\Zotero\\storage\\C6HDGJRX\\nlp-imagenet.html:text/html},
}

@article{firth_synopsis_1957,
	title = {A synopsis of linguistic theory, 1930-1955},
	pages = {10--32},
	journaltitle = {Studies in linguistic analysis},
	shortjournal = {Studies in linguistic analysis},
	author = {Firth, John},
	date = {1957},
	keywords = {notion},
	file = {lecture1-firth.pdf:C\:\\Users\\irene\\Zotero\\storage\\4TI8LAY3\\lecture1-firth.pdf:application/pdf},
}

@article{bengio_neural_2003,
	title = {A Neural Probabilistic Language Model},
	volume = {3},
	url = {https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf},
	abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difﬁcult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to ﬁght the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a signiﬁcant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach signiﬁcantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
	pages = {1137--1155},
	journaltitle = {Journal of Machine Learning Research},
	author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Jauvin, Christian},
	date = {2003-02},
	langid = {english},
	keywords = {notion},
	file = {Bengio et al. - A Neural Probabilistic Language Model.pdf:C\:\\Users\\irene\\Zotero\\storage\\WBA889I9\\Bengio et al. - A Neural Probabilistic Language Model.pdf:application/pdf},
}

@inproceedings{vaswani_attention_2017-1,
	title = {Attention is All you Need},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 {BLEU} {onEnglish}-to-German translation, improving over the existing best ensemble result by over 1 {BLEU}. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 {BLEU}, achieving a {BLEU} score of 41.1.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	urldate = {2023-12-12},
	date = {2017},
	file = {Full Text PDF:C\:\\Users\\irene\\Zotero\\storage\\AVDEZRE6\\Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@inproceedings{nozza_hate-ita_2022,
	location = {Seattle, Washington (Hybrid)},
	title = {{HATE}-{ITA}: Hate Speech Detection in Italian Social Media Text},
	url = {https://aclanthology.org/2022.woah-1.24},
	doi = {10.18653/v1/2022.woah-1.24},
	shorttitle = {{HATE}-{ITA}},
	abstract = {Online hate speech is a dangerous phenomenon that can (and should) be promptly counteracted properly. While Natural Language Processing supplies appropriate algorithms for trying to reach this objective, all research efforts are directed toward the English language. This strongly limits the classification power on non-English languages. In this paper, we test several learning frameworks for identifying hate speech in Italian text. We release {HATE}-{ITA}, a multi-language model trained on a large set of English data and available Italian datasets. {HATE}-{ITA} performs better than mono-lingual models and seems to adapt well also on language-specific slurs. We hope our findings will encourage the research in other mid-to-low resource communities and provide a valuable benchmarking tool for the Italian community.},
	eventtitle = {{WOAH} 2022},
	pages = {252--260},
	booktitle = {Proceedings of the Sixth Workshop on Online Abuse and Harms ({WOAH})},
	publisher = {Association for Computational Linguistics},
	author = {Nozza, Debora and Bianchi, Federico and Attanasio, Giuseppe},
	editor = {Narang, Kanika and Mostafazadeh Davani, Aida and Mathias, Lambert and Vidgen, Bertie and Talat, Zeerak},
	urldate = {2023-12-21},
	date = {2022-07},
	file = {Full Text PDF:C\:\\Users\\irene\\Zotero\\storage\\AVFMZMSB\\Nozza et al. - 2022 - HATE-ITA Hate Speech Detection in Italian Social .pdf:application/pdf},
}

@article{zhang_understanding_2010,
	title = {Understanding bag-of-words model: a statistical framework},
	volume = {1},
	issn = {1868-808X},
	url = {https://doi.org/10.1007/s13042-010-0001-0},
	doi = {10.1007/s13042-010-0001-0},
	abstract = {The bag-of-words model is one of the most popular representation methods for object categorization. The key idea is to quantize each extracted key point into one of visual words, and then represent each image by a histogram of the visual words. For this purpose, a clustering algorithm (e.g., K-means), is generally used for generating the visual words. Although a number of studies have shown encouraging results of the bag-of-words representation for object categorization, theoretical studies on properties of the bag-of-words model is almost untouched, possibly due to the difficulty introduced by using a heuristic clustering process. In this paper, we present a statistical framework which generalizes the bag-of-words representation. In this framework, the visual words are generated by a statistical process rather than using a clustering algorithm, while the empirical performance is competitive to clustering-based method. A theoretical analysis based on statistical consistency is presented for the proposed framework. Moreover, based on the framework we developed two algorithms which do not rely on clustering, while achieving competitive performance in object categorization when compared to clustering-based bag-of-words representations.},
	pages = {43--52},
	number = {1},
	journaltitle = {International Journal of Machine Learning and Cybernetics},
	shortjournal = {International Journal of Machine Learning and Cybernetics},
	author = {Zhang, Yin and Jin, Rong and Zhou, Zhi-Hua},
	date = {2010-12-01},
}
